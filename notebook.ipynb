{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire from forest\n",
    "\n",
    "- Authors:\n",
    "  - Axel Suárez Polo (@ggzor)\n",
    "  - Sergio Daniel Cortez Chaves (@SerCor)\n",
    "\n",
    "This notebook shows the training of a neural network to identify fire and smoke within sub-regions\n",
    "of an image. Most of this notebook shows the process of adjusting the hyperparameters using an automatic\n",
    "heuristic search (based on average precision and standard deviation).\n",
    "\n",
    "The dataset was generated by @SerCor, using [tk-tagger](https://github.com/ggzor/tk-tagger), a \n",
    "tkinter python application to interactively tag sub-regions of an image; developed to ease the\n",
    "dataset creation.\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) is used to train the neural network, but it is used\n",
    "behind a parallel multiprocessing interface with shared memory developed to speed up the training process. \n",
    "The relevant module is `experimenter.parallel`.\n",
    "\n",
    "This notebook goes step-by-step:\n",
    "  1. Configure logging and some global variables.\n",
    "  2. Load the dataset using [pandas](https://pandas.pydata.org/).\n",
    "  3. Configure the experiment ranges.\n",
    "  4. Do hyperparameter search:\n",
    "       - Optimal number of neurons and layers.\n",
    "       - Optimal number of epochs\n",
    "       - Optimal value for the learning rate.\n",
    "       - Optimal value for the momentum.\n",
    "\n",
    "Each step of the hyperparameters search takes the previous `BEST_FOREACH_PHASE` values (default 10) \n",
    "from the previous phase and searchs again in the specified range in the configuration.\n",
    "\n",
    "A heatmap is shown for each experiment, showing the relation of the optimized parameter and the mean \n",
    "validation score.\n",
    "\n",
    "Some graphics of this notebooks are not shown in the default GitHub notebook viewer, because this\n",
    "notebook makes use of the [altair](https://github.com/altair-viz/altair) visualization library. The\n",
    "recommended way to view this notebook is using [Visual Studio Code](https://code.visualstudio.com/) which\n",
    "integrates a [vega](https://github.com/vega/vega) renderer, required by the `altair` library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using POOL_SIZE=8\n"
     ]
    }
   ],
   "source": [
    "# Enable some logging capabilities\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=\"notebook.ipynb.log\", level=\"WARNING\")\n",
    "\n",
    "# Disable not useful warnings\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# The random seed to use for all the training initializations.\n",
    "RANDOM_SEED = 0\n",
    "# The number of subprocesses to use for the parallel training.\n",
    "POOL_SIZE = os.cpu_count()\n",
    "# Set to true if you want to see a test run with an approximate duration of 5 minutes.\n",
    "# Setting this value to `False` is not recommended because it can take hours.\n",
    "TEST_ENVIRONMENT = False\n",
    "\n",
    "print(f\"Using {POOL_SIZE=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load\n",
    "\n",
    "The data is loaded with pandas, we load all the `(train, test)` pairs from the `dataset/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rgb__mean_c0</th>\n",
       "      <th>rgb__mean_c1</th>\n",
       "      <th>rgb__mean_c2</th>\n",
       "      <th>rgb__stdev_c0</th>\n",
       "      <th>rgb__stdev_c1</th>\n",
       "      <th>rgb__stdev_c2</th>\n",
       "      <th>rgb__median_c0</th>\n",
       "      <th>rgb__median_c1</th>\n",
       "      <th>rgb__median_c2</th>\n",
       "      <th>rgb__cov_0</th>\n",
       "      <th>...</th>\n",
       "      <th>hsv__cov_0</th>\n",
       "      <th>hsv__cov_1</th>\n",
       "      <th>hsv__cov_2</th>\n",
       "      <th>hsv__cov_3</th>\n",
       "      <th>hsv__cov_4</th>\n",
       "      <th>hsv__cov_5</th>\n",
       "      <th>gray__mean</th>\n",
       "      <th>gray__stdev</th>\n",
       "      <th>gray__median</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.343603</td>\n",
       "      <td>-0.399791</td>\n",
       "      <td>-0.483646</td>\n",
       "      <td>0.155106</td>\n",
       "      <td>0.143023</td>\n",
       "      <td>0.015099</td>\n",
       "      <td>-0.407115</td>\n",
       "      <td>-0.510204</td>\n",
       "      <td>-0.578059</td>\n",
       "      <td>-0.108204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181116</td>\n",
       "      <td>0.032211</td>\n",
       "      <td>-0.181116</td>\n",
       "      <td>-0.388510</td>\n",
       "      <td>0.032211</td>\n",
       "      <td>-0.388510</td>\n",
       "      <td>-0.380955</td>\n",
       "      <td>0.216305</td>\n",
       "      <td>-0.480820</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.148882</td>\n",
       "      <td>-0.023510</td>\n",
       "      <td>-0.462608</td>\n",
       "      <td>-0.170048</td>\n",
       "      <td>-0.102284</td>\n",
       "      <td>-0.205109</td>\n",
       "      <td>-0.146245</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.527426</td>\n",
       "      <td>-0.496538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180401</td>\n",
       "      <td>0.149459</td>\n",
       "      <td>-0.180401</td>\n",
       "      <td>-0.204963</td>\n",
       "      <td>0.149459</td>\n",
       "      <td>-0.204963</td>\n",
       "      <td>-0.063179</td>\n",
       "      <td>-0.071409</td>\n",
       "      <td>-0.071546</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.384243</td>\n",
       "      <td>-0.314732</td>\n",
       "      <td>-0.390246</td>\n",
       "      <td>-0.084516</td>\n",
       "      <td>-0.023385</td>\n",
       "      <td>-0.078241</td>\n",
       "      <td>-0.470356</td>\n",
       "      <td>-0.436735</td>\n",
       "      <td>-0.485232</td>\n",
       "      <td>-0.386402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284572</td>\n",
       "      <td>0.137453</td>\n",
       "      <td>-0.284572</td>\n",
       "      <td>-0.140462</td>\n",
       "      <td>0.137453</td>\n",
       "      <td>-0.140462</td>\n",
       "      <td>-0.321888</td>\n",
       "      <td>0.026419</td>\n",
       "      <td>-0.441796</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.348830</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>-0.632997</td>\n",
       "      <td>-0.155131</td>\n",
       "      <td>0.059389</td>\n",
       "      <td>-0.243767</td>\n",
       "      <td>-0.351779</td>\n",
       "      <td>-0.036735</td>\n",
       "      <td>-0.696203</td>\n",
       "      <td>-0.439717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190342</td>\n",
       "      <td>0.176021</td>\n",
       "      <td>-0.190342</td>\n",
       "      <td>-0.328477</td>\n",
       "      <td>0.176021</td>\n",
       "      <td>-0.328477</td>\n",
       "      <td>-0.103680</td>\n",
       "      <td>0.039713</td>\n",
       "      <td>-0.128140</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.425188</td>\n",
       "      <td>-0.155956</td>\n",
       "      <td>-0.593988</td>\n",
       "      <td>-0.368570</td>\n",
       "      <td>-0.152293</td>\n",
       "      <td>-0.440119</td>\n",
       "      <td>-0.375494</td>\n",
       "      <td>-0.151020</td>\n",
       "      <td>-0.603376</td>\n",
       "      <td>-0.656547</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172915</td>\n",
       "      <td>0.170774</td>\n",
       "      <td>-0.172915</td>\n",
       "      <td>-0.161318</td>\n",
       "      <td>0.170774</td>\n",
       "      <td>-0.161318</td>\n",
       "      <td>-0.231420</td>\n",
       "      <td>-0.183313</td>\n",
       "      <td>-0.212932</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>0.348918</td>\n",
       "      <td>-0.459141</td>\n",
       "      <td>-0.850878</td>\n",
       "      <td>-0.960172</td>\n",
       "      <td>-0.977257</td>\n",
       "      <td>-0.980148</td>\n",
       "      <td>0.351779</td>\n",
       "      <td>-0.469388</td>\n",
       "      <td>-0.848101</td>\n",
       "      <td>-0.990671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196171</td>\n",
       "      <td>0.151483</td>\n",
       "      <td>-0.196171</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.151483</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>-0.293153</td>\n",
       "      <td>-0.976295</td>\n",
       "      <td>-0.300791</td>\n",
       "      <td>SMOKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>0.711822</td>\n",
       "      <td>-0.237385</td>\n",
       "      <td>-0.887499</td>\n",
       "      <td>-0.167512</td>\n",
       "      <td>-0.382122</td>\n",
       "      <td>-0.799823</td>\n",
       "      <td>0.889328</td>\n",
       "      <td>-0.175510</td>\n",
       "      <td>-0.898734</td>\n",
       "      <td>-0.647891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194703</td>\n",
       "      <td>0.181633</td>\n",
       "      <td>-0.194703</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.181633</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>-0.052989</td>\n",
       "      <td>-0.336232</td>\n",
       "      <td>0.031360</td>\n",
       "      <td>SMOKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.173099</td>\n",
       "      <td>-0.926741</td>\n",
       "      <td>-0.983273</td>\n",
       "      <td>-0.672096</td>\n",
       "      <td>-0.856934</td>\n",
       "      <td>0.992095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.940928</td>\n",
       "      <td>-0.991329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198950</td>\n",
       "      <td>0.151451</td>\n",
       "      <td>-0.198950</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.151451</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.306354</td>\n",
       "      <td>-0.748378</td>\n",
       "      <td>0.284923</td>\n",
       "      <td>SMOKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>0.212767</td>\n",
       "      <td>-0.332275</td>\n",
       "      <td>-0.954472</td>\n",
       "      <td>-0.583690</td>\n",
       "      <td>-0.683132</td>\n",
       "      <td>-0.841309</td>\n",
       "      <td>0.177866</td>\n",
       "      <td>-0.379592</td>\n",
       "      <td>-0.966245</td>\n",
       "      <td>-0.900064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196516</td>\n",
       "      <td>0.154068</td>\n",
       "      <td>-0.196516</td>\n",
       "      <td>-0.010151</td>\n",
       "      <td>0.154068</td>\n",
       "      <td>-0.010151</td>\n",
       "      <td>-0.240067</td>\n",
       "      <td>-0.658883</td>\n",
       "      <td>-0.285511</td>\n",
       "      <td>SMOKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>0.442095</td>\n",
       "      <td>-0.235355</td>\n",
       "      <td>-0.895595</td>\n",
       "      <td>-0.911128</td>\n",
       "      <td>-0.923450</td>\n",
       "      <td>-0.944108</td>\n",
       "      <td>0.446640</td>\n",
       "      <td>-0.248980</td>\n",
       "      <td>-0.890295</td>\n",
       "      <td>-0.985301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196077</td>\n",
       "      <td>0.151787</td>\n",
       "      <td>-0.196077</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.151787</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>-0.113511</td>\n",
       "      <td>-0.921854</td>\n",
       "      <td>-0.123725</td>\n",
       "      <td>SMOKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1534 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rgb__mean_c0  rgb__mean_c1  rgb__mean_c2  rgb__stdev_c0  rgb__stdev_c1  \\\n",
       "0        -0.343603     -0.399791     -0.483646       0.155106       0.143023   \n",
       "1        -0.148882     -0.023510     -0.462608      -0.170048      -0.102284   \n",
       "2        -0.384243     -0.314732     -0.390246      -0.084516      -0.023385   \n",
       "3        -0.348830      0.000724     -0.632997      -0.155131       0.059389   \n",
       "4        -0.425188     -0.155956     -0.593988      -0.368570      -0.152293   \n",
       "...            ...           ...           ...            ...            ...   \n",
       "1529      0.348918     -0.459141     -0.850878      -0.960172      -0.977257   \n",
       "1530      0.711822     -0.237385     -0.887499      -0.167512      -0.382122   \n",
       "1531      1.000000      0.173099     -0.926741      -0.983273      -0.672096   \n",
       "1532      0.212767     -0.332275     -0.954472      -0.583690      -0.683132   \n",
       "1533      0.442095     -0.235355     -0.895595      -0.911128      -0.923450   \n",
       "\n",
       "      rgb__stdev_c2  rgb__median_c0  rgb__median_c1  rgb__median_c2  \\\n",
       "0          0.015099       -0.407115       -0.510204       -0.578059   \n",
       "1         -0.205109       -0.146245       -0.036735       -0.527426   \n",
       "2         -0.078241       -0.470356       -0.436735       -0.485232   \n",
       "3         -0.243767       -0.351779       -0.036735       -0.696203   \n",
       "4         -0.440119       -0.375494       -0.151020       -0.603376   \n",
       "...             ...             ...             ...             ...   \n",
       "1529      -0.980148        0.351779       -0.469388       -0.848101   \n",
       "1530      -0.799823        0.889328       -0.175510       -0.898734   \n",
       "1531      -0.856934        0.992095        0.142857       -0.940928   \n",
       "1532      -0.841309        0.177866       -0.379592       -0.966245   \n",
       "1533      -0.944108        0.446640       -0.248980       -0.890295   \n",
       "\n",
       "      rgb__cov_0  ...  hsv__cov_0  hsv__cov_1  hsv__cov_2  hsv__cov_3  \\\n",
       "0      -0.108204  ...   -0.181116    0.032211   -0.181116   -0.388510   \n",
       "1      -0.496538  ...   -0.180401    0.149459   -0.180401   -0.204963   \n",
       "2      -0.386402  ...   -0.284572    0.137453   -0.284572   -0.140462   \n",
       "3      -0.439717  ...   -0.190342    0.176021   -0.190342   -0.328477   \n",
       "4      -0.656547  ...   -0.172915    0.170774   -0.172915   -0.161318   \n",
       "...          ...  ...         ...         ...         ...         ...   \n",
       "1529   -0.990671  ...   -0.196171    0.151483   -0.196171    0.004657   \n",
       "1530   -0.647891  ...   -0.194703    0.181633   -0.194703    0.004211   \n",
       "1531   -0.991329  ...   -0.198950    0.151451   -0.198950    0.004869   \n",
       "1532   -0.900064  ...   -0.196516    0.154068   -0.196516   -0.010151   \n",
       "1533   -0.985301  ...   -0.196077    0.151787   -0.196077    0.003340   \n",
       "\n",
       "      hsv__cov_4  hsv__cov_5  gray__mean  gray__stdev  gray__median    tag  \n",
       "0       0.032211   -0.388510   -0.380955     0.216305     -0.480820  OTHER  \n",
       "1       0.149459   -0.204963   -0.063179    -0.071409     -0.071546  OTHER  \n",
       "2       0.137453   -0.140462   -0.321888     0.026419     -0.441796  OTHER  \n",
       "3       0.176021   -0.328477   -0.103680     0.039713     -0.128140  OTHER  \n",
       "4       0.170774   -0.161318   -0.231420    -0.183313     -0.212932  OTHER  \n",
       "...          ...         ...         ...          ...           ...    ...  \n",
       "1529    0.151483    0.004657   -0.293153    -0.976295     -0.300791  SMOKE  \n",
       "1530    0.181633    0.004211   -0.052989    -0.336232      0.031360  SMOKE  \n",
       "1531    0.151451    0.004869    0.306354    -0.748378      0.284923  SMOKE  \n",
       "1532    0.154068   -0.010151   -0.240067    -0.658883     -0.285511  SMOKE  \n",
       "1533    0.151787    0.003340   -0.113511    -0.921854     -0.123725  SMOKE  \n",
       "\n",
       "[1534 rows x 55 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Get all the integer values from the dataset/ files.\n",
    "filenames_integers = (\n",
    "    int(m[0]) for p in Path(\"dataset/\").iterdir() if (m := re.search(\"\\d+\", str(p)))\n",
    ")\n",
    "\n",
    "# The number of partitions to use for the cross validation\n",
    "PARTITIONS = max(filenames_integers) + 1\n",
    "\n",
    "datasets = [\n",
    "    [pd.read_csv(f\"dataset/{ftype}_{i}.csv\") for ftype in [\"train\", \"test\"]]\n",
    "    for i in range(PARTITIONS)\n",
    "]\n",
    "\n",
    "datasets[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A label encoder is used to transform the tags of the data frames to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FIRE', 'OTHER', 'SMOKE'], dtype='<U5')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(\n",
    "    [tag for experiment in datasets for df in experiment for tag in df[\"tag\"].unique()]\n",
    ")\n",
    "\n",
    "label_encoder.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataExperiment` is created for each `(train, test)` dataframe pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimenter.model import DataExperiment\n",
    "\n",
    "\n",
    "def to_data_experiment(train_test_df) -> DataExperiment:\n",
    "    train, test = train_test_df\n",
    "\n",
    "    return DataExperiment(\n",
    "        train.loc[:, train.columns != \"tag\"].to_numpy(),\n",
    "        label_encoder.transform(train[\"tag\"]),\n",
    "        test.loc[:, train.columns != \"tag\"].to_numpy(),\n",
    "        label_encoder.transform(test[\"tag\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "all_experiments = [to_data_experiment(pair) for pair in datasets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete total: 17902500\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "# The number of classes within the dataset\n",
    "CLASSES = label_encoder.classes_.size\n",
    "# The number of attributes found in the dataset\n",
    "ATTRIBUTES = datasets[0][0].columns.size - 1\n",
    "\n",
    "# Number of best parameters to keep for each training phase\n",
    "BEST_FOREACH_PHASE= 15\n",
    "\n",
    "# A range for each optimizable hyperparameter\n",
    "NEURONS_RANGE = np.arange(CLASSES, ATTRIBUTES + CLASSES + 1, 1)\n",
    "LAYERS_RANGE = np.arange(1, 7 + 1, 1)\n",
    "EPOCHS_RANGE = np.arange(50, 500 + 1, 15)\n",
    "LEARNING_RATE_RANGE = np.around(np.linspace(0.001, 0.200, 50), 4)\n",
    "MOMENTUM_RANGE = np.around(np.linspace(0.01, 0.4, 30), 4)\n",
    "\n",
    "# Get the complete total number of experiments\n",
    "# Note that this is just the theoretical number of experiments that had\n",
    "# to be run if the best hyperparameters were to be found\n",
    "total_experiments = reduce(\n",
    "    lambda x, y: x * y,\n",
    "    map(\n",
    "        lambda arr: np.size(arr, 0),\n",
    "        [\n",
    "            NEURONS_RANGE,\n",
    "            LAYERS_RANGE,\n",
    "            EPOCHS_RANGE,\n",
    "            LEARNING_RATE_RANGE,\n",
    "            MOMENTUM_RANGE\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "print(f\"Complete total: {total_experiments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual configuration\n",
    "\n",
    "This variables are used to show the results in the heatmaps within using too much\n",
    "vertical space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURON_BIN_SIZE = 20\n",
    "EPOCH_BIN_SIZE = 250\n",
    "LEARNING_RATE_BIN_SIZE = 0.100\n",
    "MOMENTUM_BIN_SIZE = 0.200\n",
    "\n",
    "# Sometimes the accuracy is lower than this number, that makes some\n",
    "# details disappear, so this treshold is used to show more details.\n",
    "DETAILED_VIEW_TRESHOLD = 85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "A `ParallelExperimenter` is created with the `DataExperiment`'s that were created in the previous step.\n",
    "This class will allow us to run our training in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimenter.parallel import ParallelExperimenter\n",
    "experimenter = ParallelExperimenter(all_experiments, POOL_SIZE, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test parameters\n",
    "\n",
    "If the `TEST_ENVIRONMENT` flag is set, just a fraction of the hyperparameter tuning phases will be ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_ENVIRONMENT:\n",
    "    NEURONS_RANGE = NEURONS_RANGE[: NEURONS_RANGE.size // 4]\n",
    "    LAYERS_RANGE = LAYERS_RANGE[: NEURONS_RANGE.size // 4]\n",
    "    EPOCHS_RANGE = EPOCHS_RANGE[: EPOCHS_RANGE.size // 5]\n",
    "    LEARNING_RATE_RANGE = LEARNING_RATE_RANGE[: LEARNING_RATE_RANGE.size // 4]\n",
    "    MOMENTUM_RANGE = MOMENTUM_RANGE[: MOMENTUM_RANGE.size // 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial experimentation\n",
    "\n",
    "The initial experimentation values are set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'INITIAL_NEURONS': 3,\n",
      " 'INITIAL_LAYERS': 1,\n",
      " 'INITIAL_EPOCHS': 155,\n",
      " 'INITIAL_LEARNING_RATE': 0.1025,\n",
      " 'INITIAL_MOMENTUM': 0.2117}\n",
      "Real experiments count: 2050\n"
     ]
    }
   ],
   "source": [
    "INITIAL_NEURONS = NEURONS_RANGE[0]\n",
    "INITIAL_LAYERS = LAYERS_RANGE[0]\n",
    "INITIAL_EPOCHS = EPOCHS_RANGE[EPOCHS_RANGE.size // 4]\n",
    "INITIAL_LEARNING_RATE = LEARNING_RATE_RANGE[LEARNING_RATE_RANGE.size // 2]\n",
    "INITIAL_MOMENTUM = MOMENTUM_RANGE[MOMENTUM_RANGE.size // 2]\n",
    "\n",
    "pprint(\n",
    "    {k: v for k, v in locals().items() if k.isupper() and k.startswith(\"INITIAL\")},\n",
    "    sort_dicts=False,\n",
    ")\n",
    "\n",
    "# This is the real number of experiments that will be ran from the entire total shown above.\n",
    "real_experiments = NEURONS_RANGE.size * LAYERS_RANGE.size + BEST_FOREACH_PHASE * (\n",
    "    EPOCHS_RANGE.size + LEARNING_RATE_RANGE.size + MOMENTUM_RANGE.size\n",
    ")\n",
    "print(f\"Real experiments count: {real_experiments}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning phase\n",
    "\n",
    "Now the hyperparameters are tuned. The first step is to find the best neurons and layers parameters, where\n",
    "\"better\" is when the accuracy is higher and the standard deviation is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons and layers tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimenter.model import ModelParams\n",
    "import dataclasses\n",
    "\n",
    "initial_experiment = ModelParams(\n",
    "    INITIAL_NEURONS,\n",
    "    layers=INITIAL_LAYERS,\n",
    "    epochs=INITIAL_EPOCHS,\n",
    "    learning_rate=INITIAL_LEARNING_RATE,\n",
    "    momentum=INITIAL_MOMENTUM,\n",
    ")\n",
    "\n",
    "print(f'Running {NEURONS_RANGE.size * LAYERS_RANGE.size} experiments')\n",
    "neurons_result = experimenter.run_all(\n",
    "    [\n",
    "        dataclasses.replace(initial_experiment, neurons=neurons, layers=layers)\n",
    "        for neurons in NEURONS_RANGE\n",
    "        for layers in LAYERS_RANGE\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimenter.model import ExperimentResult\n",
    "from typing import List\n",
    "\n",
    "TOLERABLE_DEVIATION = 0.4\n",
    "NEGATIVE_DEVIATION_WEIGHT = -10\n",
    "\n",
    "\n",
    "def experiment_result_criteria(experiment: ExperimentResult) -> float:\n",
    "    deviation_factor = 0\n",
    "    if experiment.stddev > TOLERABLE_DEVIATION:\n",
    "        deviation_factor = NEGATIVE_DEVIATION_WEIGHT * experiment.stddev\n",
    "\n",
    "    return experiment.mean + deviation_factor\n",
    "\n",
    "\n",
    "def to_dataframe(experiments: List[ExperimentResult]):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                **dataclasses.asdict(result.params),\n",
    "                \"result_mean\": result.mean,\n",
    "                \"result_std\": result.stddev,\n",
    "            }\n",
    "            for result in experiments\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "neurons_result.sort(key=experiment_result_criteria, reverse=True)\n",
    "neurons_result_df = to_dataframe(neurons_result)\n",
    "\n",
    "neurons_result_df.head(BEST_FOREACH_PHASE)[[\"result_mean\", \"result_std\", \"neurons\", \"layers\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "\n",
    "def heatmap(title, source, tooltip=\"props\", **kwargs):\n",
    "    if tooltip == \"props\":\n",
    "        tooltip = source.columns.to_list()\n",
    "\n",
    "    return (\n",
    "        alt.Chart(source)\n",
    "        .mark_rect()\n",
    "        .encode(tooltip=tooltip, **kwargs)\n",
    "        .properties(title=title)\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "def binify(dataframe, param, bins):\n",
    "    return [dataframe[dataframe[param].isin(bin)] for bin in bins]\n",
    "\n",
    "\n",
    "def binified_heatmap(title, source, param, bins, tooltip=\"props\", **kwargs):\n",
    "    return alt.hconcat(\n",
    "        *(\n",
    "            heatmap(title, source[source[param].isin(bin)], tooltip=tooltip, **kwargs)\n",
    "            for bin in bins\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "neurons_result_encoding = {\"x\": \"layers:O\", \"y\": \"neurons:O\", \"color\": \"result_mean:Q\"}\n",
    "neurons_result_tooltip = [\"result_mean\", \"result_std\", \"neurons\", \"layers\"]\n",
    "NEURON_BINS = np.array_split(NEURONS_RANGE, NEURONS_RANGE.max() // NEURON_BIN_SIZE + 1)\n",
    "\n",
    "\n",
    "def binified_neurons_heatmap(title, neurons_df):\n",
    "    return binified_heatmap(\n",
    "        title,\n",
    "        neurons_df,\n",
    "        param=\"neurons\",\n",
    "        bins=NEURON_BINS,\n",
    "        tooltip=neurons_result_tooltip,\n",
    "        **neurons_result_encoding\n",
    "    )\n",
    "\n",
    "\n",
    "binified_neurons_heatmap(\"Mean precision\", neurons_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_neurons_result_df = neurons_result_df[\n",
    "    neurons_result_df.result_mean >= DETAILED_VIEW_TRESHOLD\n",
    "]\n",
    "\n",
    "binified_neurons_heatmap(\n",
    "    f\"Mean precision (>= {DETAILED_VIEW_TRESHOLD})\", range_neurons_result_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_by_neurons_layers = list(n.params for n in neurons_result[:BEST_FOREACH_PHASE])\n",
    "print(f'Running {BEST_FOREACH_PHASE * EPOCHS_RANGE.size} experiments')\n",
    "epochs_results = experimenter.run_all(\n",
    "    dataclasses.replace(p, epochs=epochs)\n",
    "    for p in best_by_neurons_layers\n",
    "    for epochs in EPOCHS_RANGE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_results.sort(key=experiment_result_criteria, reverse=True)\n",
    "epochs_results_reverse_index = {\n",
    "    (r.neurons, r.layers): i for i, r in enumerate(best_by_neurons_layers)\n",
    "}\n",
    "\n",
    "epochs_results_df = to_dataframe(epochs_results)\n",
    "\n",
    "epochs_results_df[\"src_index\"] = epochs_results_df.apply(\n",
    "    lambda row: epochs_results_reverse_index[row[\"neurons\"], row[\"layers\"]], axis=1\n",
    ")\n",
    "\n",
    "epochs_results_df.head(BEST_FOREACH_PHASE)[\n",
    "    [\"result_mean\", \"result_std\", \"neurons\", \"layers\", \"epochs\", \"src_index\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_results_encoding = {\n",
    "    \"x\": \"src_index:O\",\n",
    "    \"y\": \"epochs:O\",\n",
    "    \"color\": \"result_mean:Q\",\n",
    "}\n",
    "epochs_results_tooltip = [\"result_mean\", \"result_std\", \"neurons\", \"layers\", \"src_index\"]\n",
    "EPOCHS_BINS = np.array_split(EPOCHS_RANGE, EPOCHS_RANGE.max() // EPOCH_BIN_SIZE + 1)\n",
    "\n",
    "\n",
    "def binified_epochs_heatmap(title, epochs_df):\n",
    "    return binified_heatmap(\n",
    "        title,\n",
    "        epochs_df,\n",
    "        \"epochs\",\n",
    "        EPOCHS_BINS,\n",
    "        epochs_results_tooltip,\n",
    "        **epochs_results_encoding\n",
    "    )\n",
    "\n",
    "\n",
    "binified_epochs_heatmap(\"Mean precision\", epochs_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_epoch_results_df = epochs_results_df[\n",
    "    epochs_results_df.result_mean >= DETAILED_VIEW_TRESHOLD\n",
    "]\n",
    "\n",
    "binified_epochs_heatmap(\n",
    "    f\"Mean precision (>= {DETAILED_VIEW_TRESHOLD})\", \n",
    "    range_epoch_results_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_by_epochs = list(r.params for r in epochs_results[:BEST_FOREACH_PHASE])\n",
    "\n",
    "print(f'Running {BEST_FOREACH_PHASE * LEARNING_RATE_RANGE.size} experiments')\n",
    "\n",
    "learning_rate_results = experimenter.run_all(\n",
    "    dataclasses.replace(p, learning_rate=learning_rate)\n",
    "    for p in best_by_epochs\n",
    "    for learning_rate in LEARNING_RATE_RANGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_results.sort(key=experiment_result_criteria, reverse=True)\n",
    "learning_rate_results_reverse_index = {\n",
    "    (r.neurons, r.layers, r.epochs): i for i, r in enumerate(best_by_epochs)\n",
    "}\n",
    "\n",
    "learning_rate_results_df = to_dataframe(learning_rate_results)\n",
    "\n",
    "learning_rate_results_df[\"src_index\"] = learning_rate_results_df.apply(\n",
    "    lambda row: learning_rate_results_reverse_index[\n",
    "        row[\"neurons\"], row[\"layers\"], row[\"epochs\"]\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "learning_rate_results_df.head(BEST_FOREACH_PHASE)[\n",
    "    [\n",
    "        \"result_mean\",\n",
    "        \"result_std\",\n",
    "        \"neurons\",\n",
    "        \"layers\",\n",
    "        \"epochs\",\n",
    "        \"learning_rate\",\n",
    "        \"src_index\",\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_results_encoding = {\n",
    "    \"x\": \"src_index:O\",\n",
    "    \"y\": \"learning_rate:O\",\n",
    "    \"color\": \"result_mean:Q\",\n",
    "}\n",
    "learning_rate_results_tooltip = [\n",
    "    \"result_mean\",\n",
    "    \"result_std\",\n",
    "    \"neurons\",\n",
    "    \"layers\",\n",
    "    \"epochs\",\n",
    "    \"learning_rate\",\n",
    "    \"src_index\",\n",
    "]\n",
    "LEARNING_RATE_BINS = np.array_split(\n",
    "    LEARNING_RATE_RANGE, LEARNING_RATE_RANGE.max() // LEARNING_RATE_BIN_SIZE + 1\n",
    ")\n",
    "\n",
    "\n",
    "def binified_learning_rate_heatmap(title, lr_df):\n",
    "    return binified_heatmap(\n",
    "        title,\n",
    "        lr_df,\n",
    "        \"learning_rate\",\n",
    "        LEARNING_RATE_BINS,\n",
    "        learning_rate_results_tooltip,\n",
    "        **learning_rate_results_encoding\n",
    "    )\n",
    "\n",
    "\n",
    "binified_learning_rate_heatmap(\"Mean precision\", learning_rate_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_learning_rate_results_df = learning_rate_results_df[\n",
    "    learning_rate_results_df.result_mean >= DETAILED_VIEW_TRESHOLD\n",
    "]\n",
    "\n",
    "binified_learning_rate_heatmap(\n",
    "    f\"Mean precision (>= {DETAILED_VIEW_TRESHOLD})\", range_learning_rate_results_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_by_learning_rate = list(r.params for r in learning_rate_results[:BEST_FOREACH_PHASE])\n",
    "\n",
    "print(f'Running {BEST_FOREACH_PHASE * MOMENTUM_RANGE.size} experiments')\n",
    "\n",
    "momentum_results = experimenter.run_all(\n",
    "    dataclasses.replace(p, momentum=momentum)\n",
    "    for p in best_by_learning_rate\n",
    "    for momentum in MOMENTUM_RANGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best results\n",
    "\n",
    "The best results found are show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_results.sort(key=experiment_result_criteria, reverse=True)\n",
    "momentum_results_reverse_index = {\n",
    "    (r.neurons, r.layers, r.epochs, r.learning_rate): i for i, r in enumerate(best_by_learning_rate)\n",
    "}\n",
    "\n",
    "momentum_results_df = to_dataframe(momentum_results)\n",
    "\n",
    "momentum_results_df[\"src_index\"] = momentum_results_df.apply(\n",
    "    lambda row: momentum_results_reverse_index[\n",
    "        row[\"neurons\"], row[\"layers\"], row[\"epochs\"], row[\"learning_rate\"]\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "momentum_results_df.head(BEST_FOREACH_PHASE)[\n",
    "    [\n",
    "        \"result_mean\",\n",
    "        \"result_std\",\n",
    "        \"neurons\",\n",
    "        \"layers\",\n",
    "        \"epochs\",\n",
    "        \"learning_rate\",\n",
    "        \"momentum\",\n",
    "        \"src_index\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_results_encoding = {\n",
    "    \"x\": \"src_index:O\",\n",
    "    \"y\": \"momentum:O\",\n",
    "    \"color\": \"result_mean:Q\",\n",
    "}\n",
    "momentum_results_tooltip = [\n",
    "    \"result_mean\",\n",
    "    \"result_std\",\n",
    "    \"neurons\",\n",
    "    \"layers\",\n",
    "    \"epochs\",\n",
    "    \"learning_rate\",\n",
    "    \"momentum\",\n",
    "    \"src_index\",\n",
    "]\n",
    "MOMENTUM_BINS = np.array_split(\n",
    "    MOMENTUM_RANGE, MOMENTUM_RANGE.max() // MOMENTUM_BIN_SIZE + 1\n",
    ")\n",
    "\n",
    "\n",
    "def binified_momentum_heatmap(title, momentum_df):\n",
    "    return binified_heatmap(\n",
    "        title,\n",
    "        momentum_df,\n",
    "        \"momentum\",\n",
    "        MOMENTUM_BINS,\n",
    "        momentum_results_tooltip,\n",
    "        **momentum_results_encoding\n",
    "    )\n",
    "\n",
    "\n",
    "binified_momentum_heatmap(\"Mean precision\", momentum_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_momentum_results_df = momentum_results_df[\n",
    "    momentum_results_df.result_mean >= DETAILED_VIEW_TRESHOLD\n",
    "]\n",
    "\n",
    "binified_momentum_heatmap(\n",
    "    f\"Mean precision (>= {DETAILED_VIEW_TRESHOLD})\", range_momentum_results_df\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "345ec5c3420a484ab49a1669ed23b8f61c1f21c4ee3ef66eedcc35cb1cbbcb32"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
